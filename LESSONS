Урок 1.................................................................................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras.layers import Dense

#Данные Градусы Цельсия и Фаренгейта
c=np.array ([-40,-10,0,8,15,22,38])
f= np.arry([-40,14,32,46,59,72,100])

# Модель: простая модель линейной регрессии
model=keras.Sequential([
  Dence(units=1,input shape=[1])])
#Компиляция модели
model.compile(loss='mean squared error',
optimizer = keras.optimizers.Adam(0.1))
#Обучение модели
history = model.fit(c,f,epochs=500,verbose=0)
#График изменений функции потерь в процессе обучения
plt.plot(history.history['loss])
plt.grid(True)
plt.xlabel("Эпоха")
plt.ylabel("Потеря")
plt.title("Изменения функции потерь в процессе обучения")
#Пример: Предсказать температуру по Фаренгейту для 25 градусов Цельсия
celsius_value = 25
#Преобразование в двумерный массив
celsium_value_reshaped = np.array([celsius_value]).reshape(1,1)
#Строка преобразования:
fahrenheit_prediction = model.predict(celsius_value_reshaped)[0][0]
print(f"Прогноз для {celsius_value} градусов Цельсия: {fahrenheit_prediction} градусов Фаренгейта")
print(f "Веса модели: {model.layers[0].get_weights()[0]}")
print(f "Смещение модели:{model.layers[0].get_weights()[1]}")

Урок2..................................................................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist       
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten
from sklern.metrics import confusion_matrix,classificatin_report

# Загрузка MNIST с явным указанием типа данных 
(x_train, y_train), (x_test, y_test) = mnist.load_data(path='mnist.npz')
x_train = x_train.astype('float32')
x_test= x_test.astype('float32')

# стандартизация входных данных
x_train = x_train / 255
x_test = x_test / 255

y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

#Проверка формы данных
print ("Форма x_train:",x_train.shape)
print ("Форма y_train:",y_train.shape)
print ("Форма x_test:",x_test.shape)
print ("Форма y_test:",y_test.shape)
# отображение первых 25 изображений из обучающей выборки
plt.figure(figsize=(10,5))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_train[i], cmap=plt.cm.binary)

plt.show()

model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(128, activation='relu'),
    Dense(10, activation='softmax')
])

print(model.summary())      # вывод структуры НС в консоль

model.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])


model.fit(x_train, y_train_cat, batch_size=32, epochs=5, validation_split=0.2)
loss, accuracy = model.evaluate(x_test, y_test_cat)
print('Test accuracy:', accury)
n = 1
x = np.expand_dims(x_test[n], axis=0)
res = model.predict(x)
print( res )
print( np.argmax(res) )

plt.imshow(x_test[n], cmap=plt.cm.binary)
plt.show()

# Распознавание всей тестовой выборки
pred = model.predict(x_test)
pred = np.argmax(pred, axis=1)

print(pred.shape)

print(pred[:20])
print(y_test[:20])

# Выделение неверных вариантов
mask = pred == y_test
print(mask[:10])
if np.any(~mask):
   x_false = x_test[~mask]
   y_false = x_test[~mask]
   print(x_false.shape)

# Вывод первых 25 неверных результатов
plt.figure(figsize=(10,5))
for i in range(25):
    plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_false[i], cmap=plt.cm.binary)

   plt.show()
else:
print("Нет неверных классификаций!")

# Дополнительная оценка:Отчет классификации 
print ( classification_report(y_test,pred))
# Дополнительная оценка: матрица неточностей
print(confusion_matrix(y_test, pred)

Урок3.................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Dropout
from tensorflow.keras.callbacks import EarlyStopping # Ранняя остановка

# Загрузка MNIST
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Нормализация данных
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

# Определение размеров выборок
limit = 5000
x_train_data = x_train[:limit]
y_train_data = y_train_cat[:limit]

x_valid = x_train[limit:limit*2]
y_valid = y_train_cat[limit:limit*2]

# Архитектура модели
model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(300, activation='relu'),
    Dropout(0.3),  # Уменьшаем Dropout до 0.3
    Dense(10, activation='softmax')
])

# Компиляция модели
model.compile(optimizer='adam',
             loss='categorical_crossentropy',
             metrics=['accuracy'])

# Ранняя остановка
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Обучение модели
his = model.fit(x_train_data, y_train_data, epochs=50, batch_size=32, validation_data=(x_valid, y_valid), callbacks=[early_stopping])

# График обучения
plt.plot(his.history['loss'], label='Обучающая выборка')
plt.plot(his.history['val_loss'], label='Валидационная выборка')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.title('Кривая обучения')
plt.show()

# Оценка на тестовой выборке
loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print('Точность на тестовой выборке: %f' % (accuracy))
 

Batch Normalization (батч-нормализация)
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Загрузка данных
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Нормализация данных
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Преобразование меток в категориальный вид
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

# Разделение на обучающую и валидационную выборки
limit = 5000
x_train_data = x_train[:limit]
y_train_data = y_train_cat[:limit]

x_valid = x_train[limit:limit*2]
y_valid = y_train_cat[limit:limit*2]

# Создание модели
model = keras.Sequential([
    Flatten(input_shape=(28, 28)),
    Dense(300),  # Без activation, чтобы BN стоял правильно
    BatchNormalization(),
    keras.layers.Activation('relu'),
    Dense(10, activation='softmax')  # Финальный softmax слой
])

# Компиляция модели
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Обучение модели
his = model.fit(x_train_data, y_train_data, epochs=100, batch_size=32,
                validation_data=(x_valid, y_valid), callbacks=[early_stopping])

# Графики обучения
plt.plot(his.history['loss'], label='Обучающая выборка')
plt.plot(his.history['val_loss'], label='Валидационная выборка')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.show()

Урок4......................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Dropout, Conv2D, MaxPooling2D, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping

# Загрузка данных
(x_train, y_train), (x_test, y_test) = mnist.load_data()

# Нормализация данных
x_train = x_train.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Преобразование меток в категориальный вид
y_train_cat = keras.utils.to_categorical(y_train, 10)
y_test_cat = keras.utils.to_categorical(y_test, 10)

# Расширение размерности для соответствия входному формату Conv2D
x_train = np.expand_dims(x_train, axis=3)
x_test = np.expand_dims(x_test, axis=3)

print( x_train.shape )

# Создание модели
model = keras.Sequential([
    Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(28, 28, 1)),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    Conv2D(64, (3,3), padding='same', activation='relu'),
    BatchNormalization(),
    MaxPooling2D((2, 2), strides=2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5), # Dropout для регуляризации
    Dense(10, activation='softmax')
])

# Компиляция модели
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

# Обучение модели
his = model.fit(x_train, y_train_cat, batch_size=32, epochs=20, validation_split=0.2, callbacks=[early_stopping])

# Оценка на тестовой выборке
loss, accuracy = model.evaluate(x_test, y_test_cat, verbose=0)
print('Точность на тестовой выборке: %f' % (accuracy))

# Графики обучения
plt.plot(his.history['accuracy'], label='Обучающая выборка (Точность)')
plt.plot(his.history['val_accuracy'], label='Валидационная выборка (Точность)')
plt.xlabel('Эпоха')
plt.ylabel('Точность')
plt.legend()
plt.show()

plt.plot(his.history['loss'], label='Обучающая выборка (Потеря)')
plt.plot(his.history['val_loss'], label='Валидационная выборка (Потеря)')
plt.xlabel('Эпоха')
plt.ylabel('Функция потерь')
plt.legend()
plt.show()
Урок5........................................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from google.colab import files
from io import BytesIO
from PIL import Image

# Загрузка предварительно обученной модели VGG16
model = keras.applications.VGG16()

# Загрузка файла
try:
    uploaded = files.upload()
    img = Image.open(BytesIO(uploaded['2.jpg']))

    # Проверка и изменение размера изображения, если необходимо
    img = img.resize((224, 224))  # Явно изменяем размер до 224x224
    plt.imshow( img )
    plt.show()

    # Преобразование изображения в NumPy-массив и предварительная обработка
    img = np.array(img)
    x = keras.applications.vgg16.preprocess_input(img)
    print(x.shape)
    x = np.expand_dims(x, axis=0)

    # Прогнозирование класса изображения
    res = model.predict(x)
    predicted_class_index = np.argmax(res)

    # Декодирование результата
    decoded_predictions = keras.applications.vgg16.decode_predictions(res, top=5)[0] # top=5 возвращает 5 лучших результатов

    print("Предсказания:")
    for i, (imagenet_id, label, probability) in enumerate(decoded_predictions):
        print(f"{i+1}: {label} ({probability:.4f})")

    # Вывод наиболее вероятного класса
    print(f"\nНаиболее вероятный класс: {decoded_predictions[0][1]}")

except FileNotFoundError:
    print("Файл '2.jpg' не найден. Пожалуйста, загрузите изображение.")
except Exception as e:
    print(f"Произошла ошибка: {e}")
Урок 6..............................................................................................................................................................
import numpy as np
import matplotlib.pyplot as plt
from google.colab import files
from io import BytesIO
from PIL import Image

import tensorflow as tf
from tensorflow import keras

upl = files.upload()
img = Image.open(BytesIO(upl['1.jpg']))
img_style = Image.open(BytesIO(upl['3.jpg']))

plt.subplot(1, 2, 1)
plt.imshow( img )
plt.subplot(1, 2, 2)
plt.imshow( img_style )
plt.show()

x_img = keras.applications.vgg19.preprocess_input( np.expand_dims(img, axis=0) )
x_style = keras.applications.vgg19.preprocess_input(np.expand_dims(img_style, axis=0))

def deprocess_img(processed_img):
  x = processed_img.copy()
  if len(x.shape) == 4:
    x = np.squeeze(x, 0)
  assert len(x.shape) == 3, ("Input to deprocess image must be an image of "
                             "dimension [1, height, width, channel] or [height, width, channel]")
  if len(x.shape) != 3:
    raise ValueError("Invalid input to deprocessing image")
  
  # perform the inverse of the preprocessing step
  x[:, :, 0] += 103.939
  x[:, :, 1] += 116.779
  x[:, :, 2] += 123.68
  x = x[:, :, ::-1]

  x = np.clip(x, 0, 255).astype('uint8')
  return x

vgg = keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')
vgg.trainable = False 

# Content layer where will pull our feature maps
content_layers = ['block5_conv2'] 

# Style layer we are interested in
style_layers = ['block1_conv1',
                'block2_conv1',
                'block3_conv1', 
                'block4_conv1', 
                'block5_conv1'
               ]

num_content_layers = len(content_layers)
num_style_layers = len(style_layers)

style_outputs = [vgg.get_layer(name).output for name in style_layers]
content_outputs = [vgg.get_layer(name).output for name in content_layers]
model_outputs = style_outputs + content_outputs

print(vgg.input)
for m in model_outputs:
  print(m)

model = keras.models.Model(vgg.input, model_outputs)
for layer in model.layers:
    layer.trainable = False

print(model.summary())      # вывод структуры НС в консоль

def get_feature_representations(model):
  # batch compute content and style features
  style_outputs = model(x_style)
  content_outputs = model(x_img)
  
  # Get the style and content feature representations from our model  
  style_features = [style_layer[0] for style_layer in style_outputs[:num_style_layers]]
  content_features = [content_layer[0] for content_layer in content_outputs[num_style_layers:]]
  return style_features, content_features

def get_content_loss(base_content, target):
  return tf.reduce_mean(tf.square(base_content - target))

def gram_matrix(input_tensor):
  # We make the image channels first 
  channels = int(input_tensor.shape[-1])
  a = tf.reshape(input_tensor, [-1, channels])
  n = tf.shape(a)[0]
  gram = tf.matmul(a, a, transpose_a=True)
  return gram / tf.cast(n, tf.float32)

def get_style_loss(base_style, gram_target):
  gram_style = gram_matrix(base_style)
  
  return tf.reduce_mean(tf.square(gram_style - gram_target))

def compute_loss(model, loss_weights, init_image, gram_style_features, content_features):
  style_weight, content_weight = loss_weights
  
  model_outputs = model(init_image)
  
  style_output_features = model_outputs[:num_style_layers]
  content_output_features = model_outputs[num_style_layers:]
  
  style_score = 0
  content_score = 0

  # Accumulate style losses from all layers
  # Here, we equally weight each contribution of each loss layer
  weight_per_style_layer = 1.0 / float(num_style_layers)
  for target_style, comb_style in zip(gram_style_features, style_output_features):
    style_score += weight_per_style_layer * get_style_loss(comb_style[0], target_style)
    
  # Accumulate content losses from all layers 
  weight_per_content_layer = 1.0 / float(num_content_layers)
  for target_content, comb_content in zip(content_features, content_output_features):
    content_score += weight_per_content_layer* get_content_loss(comb_content[0], target_content)
  
  style_score *= style_weight
  content_score *= content_weight

  # Get total loss
  loss = style_score + content_score 
  return loss, style_score, content_score

num_iterations=100
content_weight=1e3
style_weight=1e-2

style_features, content_features = get_feature_representations(model)
gram_style_features = [gram_matrix(style_feature) for style_feature in style_features]

init_image = np.copy(x_img)
init_image = tf.Variable(init_image, dtype=tf.float32)

opt = tf.compat.v1.train.AdamOptimizer(learning_rate=2, beta1=0.99, epsilon=1e-1)
iter_count = 1
best_loss, best_img = float('inf'), None
loss_weights = (style_weight, content_weight)

cfg = {
      'model': model,
      'loss_weights': loss_weights,
      'init_image': init_image,
      'gram_style_features': gram_style_features,
      'content_features': content_features
}

norm_means = np.array([103.939, 116.779, 123.68])
min_vals = -norm_means
max_vals = 255 - norm_means
imgs = []

for i in range(num_iterations):
    with tf.GradientTape() as tape: 
       all_loss = compute_loss(**cfg)
    
    loss, style_score, content_score = all_loss
    grads = tape.gradient(loss, init_image)

    opt.apply_gradients([(grads, init_image)])
    clipped = tf.clip_by_value(init_image, min_vals, max_vals)
    init_image.assign(clipped)
    
    if loss < best_loss:
      # Update best loss and best image from total loss. 
      best_loss = loss
      best_img = deprocess_img(init_image.numpy())

      # Use the .numpy() method to get the concrete numpy array
      plot_img = deprocess_img(init_image.numpy())
      imgs.append(plot_img)
      print('Iteration: {}'.format(i))

plt.imshow(best_img)
print(best_loss)

image = Image.fromarray(best_img.astype('uint8'), 'RGB')
image.save("result.jpg")
files.download("result.jpg")

Урок 7...............................................................................................

from keras.layers import Conv2D, UpSampling2D, InputLayer
from keras.models import Sequential
from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img # <---- ИСПРАВЛЕНИЕ ЗДЕСЬ
from skimage.color import rgb2lab, lab2rgb
from skimage.io import imsave
import numpy as np
from google.colab import files
from io import BytesIO
from PIL import Image
import matplotlib.pyplot as plt

# --- Функции ---
def processed_image(img):
    """Преобразует изображение в формат, подходящий для модели."""
    image = img.resize((256, 256), Image.BILINEAR)
    image = np.array(image, dtype=float)
    size = image.shape
    lab = rgb2lab(1.0/255*image)  # Нормировка и преобразование в Lab
    X, Y = lab[:,:,0], lab[:,:,1:]

    Y /= 128  # Нормируем выходные значение в диапазон от -1 до 1
    X = X.reshape(1, size[0], size[1], 1)
    Y = Y.reshape(1, size[0], size[1], 2)
    return X, Y, size

def postprocess_image(X, ab, size):
    """Преобразует выходные данные модели обратно в RGB."""
    cur = np.zeros((size[0], size[1], 3))
    cur[:,:,0] = np.clip(X[0][:,:,0], 0, 100)
    cur[:,:,1:] = ab
    return lab2rgb(cur)

# --- Модель ---
model = Sequential()
model.add(InputLayer(input_shape=(256, 256, 1))) # фиксированный размер
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same', strides=2))
model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(Conv2D(2, (3, 3), activation='tanh', padding='same'))
model.add(UpSampling2D((2, 2)))

model.compile(optimizer='adam', loss='mse')
model.summary() # Полезно посмотреть структуру

# --- Обучение на одном изображении---
upl = files.upload()
names = list(upl.keys())
img = Image.open(BytesIO(upl[names[0]]))
X, Y, size = processed_image(img)
model.fit(x=X, y=Y, batch_size=1, epochs=50) # ОЧЕНЬ МАЛО ДАННЫХ!

# --- Предсказание ---
upl = files.upload()
names = list(upl.keys())
img = Image.open(BytesIO(upl[names[0]]))
X, Y, size = processed_image(img) # Предварительная обработка как и при обучении

output = model.predict(X)

output *= 128 # Обратная нормализация
min_vals, max_vals = -128, 127
ab = np.clip(output[0], min_vals, max_vals)

# --- Отображение ---
colored_image = postprocess_image(X, ab, size)

plt.figure(figsize=(10,5))
plt.subplot(1, 2, 1)
plt.imshow(img)
plt.title("Исходное изображение")
plt.subplot(1, 2, 2)
plt.imshow(colored_image)  # lab2rgb уже встроен в postprocess_image
plt.title("Раскрашенное изображение")
plt.show()
Урок8.............................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import re

from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from google.colab import files # Добавляем импорт

# Загрузка файла
try:
    uploaded = files.upload()
    file_name = next(iter(uploaded)) # Получаем имя загруженного файла

    # Загрузка и предобработка текста
    with open(file_name, 'r', encoding='utf-8') as f: # Используем имя загруженного файла
        text = f.read()
        text = text.replace('\ufeff', '')  # убираем первый невидимый символ
        text = re.sub(r'[^А-я ]', '', text)  # заменяем все символы кроме кириллицы на пустые символы

    # Параметры
    num_characters = 34  # 33 буквы + пробел
    inp_chars = 6

    # Токенизация
    tokenizer = Tokenizer(num_words=num_characters, char_level=True, oov_token="<UNK>")  # Добавляем oov_token
    tokenizer.fit_on_texts([text])  # формируем токены
    print(tokenizer.word_index)

    # Формирование данных для обучения
    data = tokenizer.texts_to_matrix(text)  # Преобразуем текст в OHE
    n = data.shape[0] - inp_chars  # Количество примеров

    X = np.array([data[i:i + inp_chars, :] for i in range(n)])
    Y = data[inp_chars:]  # предсказание следующего символа

    print(data.shape)

    # Создание модели
    model = Sequential()
    model.add(Input((inp_chars, num_characters)))
    model.add(SimpleRNN(128, activation='tanh')) # RNN слой
    model.add(Dense(num_characters, activation='softmax'))
    model.summary()

    # Компиляция и обучение модели
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

    history = model.fit(X, Y, batch_size=32, epochs=100)

    # Функция для генерации текста
    def buildPhrase(inp_str, str_len=50):
        """Генерирует текст на основе входной строки."""
        result = inp_str
        for i in range(str_len):
            # Берем последние inp_chars символов
            tail = result[-inp_chars:]

            # Токенизируем последние символы
            tokens = tokenizer.texts_to_sequences([tail])  # to_sequences вместо texts_to_matrix

            #Если есть неизвестные токены, то заменяем их на токен для неизвестного символа
            tokens = [[tokenizer.word_index.get(c, tokenizer.word_index["<UNK>"]) for c in seq] for seq in tokens]

            # Преобразуем в OHE
            x = to_categorical(tokens[0], num_classes=num_characters) # to_categorical из keras.utils

            # Дополняем нулями, если длина меньше inp_chars (маловероятно, но для надежности)
            if x.shape[0] < inp_chars:
                padding = np.zeros((inp_chars - x.shape[0], num_characters))
                x = np.concatenate([padding, x])

            # Изменяем форму для подачи в модель
            x = np.expand_dims(x, axis=0)

            # Предсказываем следующий символ
            pred = model.predict(x)
            next_char_index = np.argmax(pred)
            next_char = tokenizer.index_word.get(next_char_index, "<UNK>")  # Получаем символ по индексу, обрабатываем UNK

            result += next_char # Добавляем к результату

        return result

    # Пример использования
    res = buildPhrase("утренн")
    print(res)

except FileNotFoundError as e:
    print(f"Ошибка: Файл не найден. Пожалуйста, загрузите файл 'train_data_true'.")
except Exception as e:
    print(f"Произошла ошибка: {e}")
Урок 9.....................................................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import re

from tensorflow.keras.layers import Dense, SimpleRNN, Input
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.utils import to_categorical
from google.colab import files # Добавляем импорт

# Загрузка файла
try:
    uploaded = files.upload()
    file_name = next(iter(uploaded)) # Получаем имя загруженного файла

    # Загрузка и предобработка текста
    with open(file_name, 'r', encoding='utf-8') as f: # Используем имя загруженного файла
        text = f.read()
        text = text.replace('\ufeff', '')  # убираем первый невидимый символ
        text = re.sub(r'[^А-я ]', '', text)  # заменяем все символы кроме кириллицы на пустые символы

    # Параметры
    num_characters = 34  # 33 буквы + пробел
    inp_chars = 6

    # Токенизация
    tokenizer = Tokenizer(num_words=num_characters, char_level=True, oov_token="<UNK>")  # Добавляем oov_token
    tokenizer.fit_on_texts([text])  # формируем токены
    print(tokenizer.word_index)

    # Формирование данных для обучения
    data = tokenizer.texts_to_matrix(text)  # Преобразуем текст в OHE
    n = data.shape[0] - inp_chars  # Количество примеров

    X = np.array([data[i:i + inp_chars, :] for i in range(n)])
    Y = data[inp_chars:]  # предсказание следующего символа

    print(data.shape)

    # Создание модели
    model = Sequential()
    model.add(Input((inp_chars, num_characters)))
    model.add(SimpleRNN(128, activation='tanh')) # RNN слой
    model.add(Dense(num_characters, activation='softmax'))
    model.summary()

    # Компиляция и обучение модели
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

    history = model.fit(X, Y, batch_size=32, epochs=100)

    # Функция для генерации текста
    def buildPhrase(inp_str, str_len=50):
        """Генерирует текст на основе входной строки."""
        result = inp_str
        for i in range(str_len):
            # Берем последние inp_chars символов
            tail = result[-inp_chars:]

            # Токенизируем последние символы
            tokens = tokenizer.texts_to_sequences([tail])  # to_sequences вместо texts_to_matrix

            #Если есть неизвестные токены, то заменяем их на токен для неизвестного символа
            tokens = [[tokenizer.word_index.get(c, tokenizer.word_index["<UNK>"]) for c in seq] for seq in tokens]

            # Преобразуем в OHE
            x = to_categorical(tokens[0], num_classes=num_characters) # to_categorical из keras.utils

            # Дополняем нулями, если длина меньше inp_chars (маловероятно, но для надежности)
            if x.shape[0] < inp_chars:
                padding = np.zeros((inp_chars - x.shape[0], num_characters))
                x = np.concatenate([padding, x])

            # Изменяем форму для подачи в модель
            x = np.expand_dims(x, axis=0)

            # Предсказываем следующий символ
            pred = model.predict(x)
            next_char_index = np.argmax(pred)
            next_char = tokenizer.index_word.get(next_char_index, "<UNK>")  # Получаем символ по индексу, обрабатываем UNK

            result += next_char # Добавляем к результату

        return result

    # Пример использования
    res = buildPhrase("утренн")
    print(res)

except FileNotFoundError as e:
    print(f"Ошибка: Файл не найден. Пожалуйста, загрузите файл 'train_data_true'.")
except Exception as e:
    print(f"Произошла ошибка: {e}")
Урок10..........................................................................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy
from google.colab import files

try:
    # Загрузка файлов
    uploaded = files.upload()
    texts_true_file = next(iter(uploaded))
    texts_false_file = next(iter(uploaded)) #Загружаем второй файл

    # Чтение данных
    with open(texts_true_file, 'r', encoding='utf-8') as f:
        texts_true = f.readlines()
        texts_true[0] = texts_true[0].replace('\ufeff', '')

    with open(texts_false_file, 'r', encoding='utf-8') as f:
        texts_false = f.readlines()
        texts_false[0] = texts_false[0].replace('\ufeff', '')

    texts = texts_true + texts_false
    count_true = len(texts_true)
    count_false = len(texts_false)
    total_lines = count_true + count_false
    print(count_true, count_false, total_lines)

    # Параметры
    maxWordsCount = 1000
    max_text_len = 20  # Увеличили длину

    # Токенизация
    tokenizer = Tokenizer(num_words=maxWordsCount,
                          filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»',
                          lower=True, split=' ', char_level=False,
                          oov_token="<UNK>")  # Добавили oov_token
    tokenizer.fit_on_texts(texts)

    dist = list(tokenizer.word_counts.items())
    print(dist[:10])
    print(texts[0][:100])

    # Формирование последовательностей и padding
    data = tokenizer.texts_to_sequences(texts)
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(data_pad)

    print(list(tokenizer.word_index.items()))

    # Формирование X и Y
    X = data_pad
    Y = np.array([[1, 0]] * count_true + [[0, 1]] * count_false)
    print(X.shape, Y.shape)

    # Перемешивание данных
    indeces = np.random.choice(X.shape[0], size=X.shape[0], replace=False)
    X = X[indeces]
    Y = Y[indeces]

    # Создание модели
    model = Sequential()
    model.add(Embedding(maxWordsCount, 128, input_length=max_text_len))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))  # Добавили Dropout
    model.add(LSTM(64))
    model.add(Dropout(0.5))  # Добавили Dropout
    model.add(Dense(2, activation='softmax'))
    model.summary()

    # Компиляция и обучение
    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))

    history = model.fit(X, Y, batch_size=32, epochs=50)

    # Обратный словарь
    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

    def sequence_to_text(list_of_indices):
        words = [reverse_word_map.get(letter) or '<UNK>' for letter in list_of_indices] # обработка UNK
        return ' '.join(words) # соединяем в строку

    # Пример использования
    t = "Я люблю позитивное настроение".lower()
    data = tokenizer.texts_to_sequences([t])
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(sequence_to_text(data_pad[0])) # Правильно используем data_pad

    res = model.predict(data_pad)
    print(res, np.argmax(res), sep='\n')

except Exception as e:
    print(f"Произошла ошибка: {e}")
Урок11.......................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import re

from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Embedding
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from google.colab import files

try:
    # Загрузка файлов
    uploaded = files.upload()
    texts_true_file = next(iter(uploaded))  # Get the first uploaded file
    texts_false_file = next(iter(uploaded))  # Get the second uploaded file

    # Чтение данных
    with open(texts_true_file, 'r', encoding='utf-8') as f:
        texts_true = f.readlines()
        texts_true[0] = texts_true[0].replace('\ufeff', '')

    with open(texts_false_file, 'r', encoding='utf-8') as f:
        texts_false = f.readlines()
        texts_false[0] = texts_false[0].replace('\ufeff', '')

    texts = texts_true + texts_false
    count_true = len(texts_true)
    count_false = len(texts_false)
    total_lines = count_true + count_false
    print(count_true, count_false, total_lines)

    maxWordsCount = 1000
    tokenizer = Tokenizer(num_words=maxWordsCount, filters='!–"—#$%&amp;()*+,-./:;<=>?@[\\]^_`{|}~\t\n\r«»', lower=True,
                          split=' ', char_level=False, oov_token='<UNK>')  # Add oov_token
    tokenizer.fit_on_texts(texts)

    dist = list(tokenizer.word_counts.items())
    print(dist[:10])
    print(texts[0][:100])

    max_text_len = 20 #Увеличил длину

    data = tokenizer.texts_to_sequences(texts)
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(data_pad)

    print(list(tokenizer.word_index.items()))

    X = data_pad
    Y = np.array([[1, 0]] * count_true + [[0, 1]] * count_false)
    print(X.shape, Y.shape)

    indeces = np.random.choice(X.shape[0], size=X.shape[0], replace=False)
    X = X[indeces]
    Y = Y[indeces]

    model = Sequential()
    model.add(Embedding(maxWordsCount, 128, input_length=max_text_len))
    model.add(GRU(128, return_sequences=True))
    model.add(Dropout(0.5)) #Добавляем Dropout
    model.add(GRU(64))
    model.add(Dropout(0.5)) #Добавляем Dropout
    model.add(Dense(2, activation='softmax'))
    model.summary()

    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=Adam(0.0001))

    history = model.fit(X, Y, batch_size=32, epochs=50)

    reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))

    def sequence_to_text(list_of_indices):
        words = [reverse_word_map.get(letter) or '<UNK>' for letter in list_of_indices] #Обрабатываем UNK
        return ' '.join(words)

    t = "Это шедевр изобразительного искусства".lower()
    data = tokenizer.texts_to_sequences([t])
    data_pad = pad_sequences(data, maxlen=max_text_len)
    print(sequence_to_text(data_pad[0])) #Используем data_pad[0]

    res = model.predict(data_pad)
    print(res, np.argmax(res), sep='\n')

except Exception as e:
    print(f"Произошла ошибка: {e}")
Урок12.....................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

N = 10000
data = np.array([np.sin(x/20) for x in range(N)]) + 0.1*np.random.randn(N)
plt.plot(data[:100])

off = 3
length = off*2+1
X = np.array([ np.diag(np.hstack((data[i:i+off], data[i+off+1:i+length]))) for i in range(N-length)])
Y = data[off:N-off-1]
print(X.shape, Y.shape, sep='\n')

model = Sequential()
model.add(Input((length-1, length-1)))
model.add( Bidirectional(GRU(2)) )
model.add(Dense(1, activation='linear'))
model.summary()

model.compile(loss='mean_squared_error', optimizer=Adam(0.01))

history = model.fit(X, Y, batch_size=32, epochs=10)

M = 200
XX = np.zeros(M)
XX[:off] = data[:off]
for i in range(M-off-1):
  x = np.diag( np.hstack( (XX[i:i+off], data[i+off+1:i+length])) )
  x = np.expand_dims(x, axis=0)
  y = model.predict(x)
  XX[i+off] = y

plt.plot(XX[:M])
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

N = 10000
data = np.array([np.sin(x/20) for x in range(N)]) + 0.1*np.random.randn(N)
plt.plot(data[:100])

off = 3
length = off*2+1
X = np.array([ np.diag(np.hstack((data[i:i+off], data[i+off+1:i+length]))) for i in range(N-length)])
Y = data[off:N-off-1]
print(X.shape, Y.shape, sep='\n')

model = Sequential()
model.add(Input((length-1, length-1)))
model.add( Bidirectional(GRU(2)) )
model.add(Dense(1, activation='linear'))
model.summary()

model.compile(loss='mean_squared_error', optimizer=Adam(0.01))

history = model.fit(X, Y, batch_size=32, epochs=10)

M = 200
XX = np.zeros(M)
XX[:off] = data[:off]
for i in range(M-off-1):
  x = np.diag( np.hstack( (XX[i:i+off], data[i+off+1:i+length])) )
  x = np.expand_dims(x, axis=0)
  y = model.predict(x)
  XX[i+off] = y

plt.plot(XX[:M])
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.layers import Dense, GRU, Input, Dropout, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.optimizers import Adam

N = 10000
data = np.array([np.sin(x/20) for x in range(N)]) + 0.1*np.random.randn(N)
plt.plot(data[:100])

off = 3
length = off*2+1
X = np.array([ np.diag(np.hstack((data[i:i+off], data[i+off+1:i+length]))) for i in range(N-length)])
Y = data[off:N-off-1]
print(X.shape, Y.shape, sep='\n')

model = Sequential()
model.add(Input((length-1, length-1)))
model.add( Bidirectional(GRU(2)) )
model.add(Dense(1, activation='linear'))
model.summary()

model.compile(loss='mean_squared_error', optimizer=Adam(0.01))

history = model.fit(X, Y, batch_size=32, epochs=10)

M = 200
XX = np.zeros(M)
XX[:off] = data[:off]
for i in range(M-off-1):
  x = np.diag( np.hstack( (XX[i:i+off], data[i+off+1:i+length])) )
  x = np.expand_dims(x, axis=0)
  y = model.predict(x)
  XX[i+off] = y

plt.plot(XX[:M])
plt.plot(data[:M])
Урок13..............................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist  # библиотека базы выборок Mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# стандартизация входных данных
x_train = x_train / 255
x_test = x_test / 255

x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

batch_size = 100

input_img = Input((28, 28, 1))
x = Flatten()(input_img)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
encoded = Dense(49, activation='relu')(x)

d = Dense(64, activation='relu')(encoded)
d = Dense(28 * 28, activation='sigmoid')(d)
decoded = Reshape((28, 28, 1))(d)

autoencoder = keras.Model(input_img, decoded, name="autoencoder")
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

autoencoder.fit(x_train, x_train,
                epochs=20,
                batch_size=batch_size,
                shuffle=True)

n = 10

imgs = x_test[:n]
decoded_imgs = autoencoder.predict(x_test[:n], batch_size=n)

plt.figure(figsize=(n, 2))
for i in range(n):
  ax = plt.subplot(2, n, i + 1)
  plt.imshow(imgs[i].squeeze(), cmap='gray')
  ax.get_xaxis().set_visible(False)
  ax.get_yaxis().set_visible(False)

  ax2 = plt.subplot(2, n, i + n + 1)
  plt.imshow(decoded_imgs[i].squeeze(), cmap='gray')
  ax2.get_xaxis().set_visible(False)
  ax2.get_yaxis().set_visible(False)

plt.show()

def plot_digits(*images):
  images = [x.squeeze() for x in images]
  n = images[0].shape[0]  # число изображений

  plt.figure(figsize=(n, len(images)))
  for j in range(n):
    for i in range(len(images)):
      ax = plt.subplot(len(images), n, i * n + j + 1)
      plt.imshow(images[i][j])
      plt.gray()
      ax.get_xaxis().set_visible(False)
      ax.get_yaxis().set_visible(False)

  plt.show()


def plot_homotopy(frm, to, n=10, autoencoder=None):
  z = np.zeros(([n] + list(frm.shape)))
  for i, t in enumerate(np.linspace(0., 1., n)):
    z[i] = frm * (1 - t) + to * t  # Гомотопия по прямой
  if autoencoder:
    plot_digits(autoencoder.predict(z, batch_size=n))
  else:
    plot_digits(z)


frm, to = x_test[y_test == 5][1:3]
plot_homotopy(frm, to)
plot_homotopy(frm, to, autoencoder=autoencoder)
Урок14......................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, BatchNormalization

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# стандартизация входных данных
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

input_img = Input(shape=(28, 28, 1))
x = Flatten()(input_img)
x = Dense(128, activation='relu')(x)
x = Dense(64, activation='relu')(x)
encoded = Dense(2, activation='linear')(x)
encoded = BatchNormalization()(encoded)  # Нормализация латентного пространства

input_enc = Input(shape=(2,))
d = Dense(64, activation='relu')(input_enc)
d = Dense(28 * 28, activation='sigmoid')(d)
decoded = Reshape((28, 28, 1))(d)

encoder = keras.Model(input_img, encoded, name="encoder")
decoder = keras.Model(input_enc, decoded, name="decoder")
autoencoder = keras.Model(input_img, decoder(encoder(input_img)), name="autoencoder")
autoencoder.compile(optimizer='adam', loss='mean_squared_error')

autoencoder.fit(x_train, x_train,
                epochs=10,
                batch_size=64,
                shuffle=True)

# Визуализация латентного пространства с метками классов
h = encoder.predict(x_test)
plt.figure(figsize=(8, 6))
scatter = plt.scatter(h[:, 0], h[:, 1], c=y_test, cmap='viridis')  # Раскрашиваем точки в соответствии с классом
plt.colorbar(scatter)  # Добавляем цветовую шкалу
plt.title("Latent Space Visualization")
plt.xlabel("Latent Dimension 1")
plt.ylabel("Latent Dimension 2")
plt.show()

# Визуализация изображения из латентного пространства
img = decoder.predict(np.expand_dims([0, 0], axis=0)) #Предсказание для точки 0,0
plt.imshow(img.squeeze(), cmap='gray')
plt.title("Decoded Image from Latent Space")
plt.show()

# Интерполяция в латентном пространстве
num_interpolation_steps = 10
point_1 = [-2, -2] #Задаем координаты точки 1
point_2 = [2, 2] #Задаем координаты точки 2

interpolated_vectors = np.zeros((num_interpolation_steps, 2)) #Массив для хранения результатов

for i in range(num_interpolation_steps):
    alpha = i / (num_interpolation_steps - 1)  # Параметр интерполяции
    interpolated_vectors[i] = (1 - alpha) * np.array(point_1) + alpha * np.array(point_2) #Интерполяция между двумя точками

interpolated_images = decoder.predict(interpolated_vectors) #Создаем изображения на основе точек

# Визуализация интерполированных изображений
plt.figure(figsize=(15, 3))
for i in range(num_interpolation_steps):
    ax = plt.subplot(1, num_interpolation_steps, i + 1)
    plt.imshow(interpolated_images[i].squeeze(), cmap='gray') #Выводим изображения
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.suptitle('Interpolation in Latent Space') #Заголовок
plt.show()
Урок 15...............................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from tensorflow import keras
import tensorflow.keras.backend as K
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, Lambda, BatchNormalization, Dropout, Layer
from tensorflow.keras.layers import concatenate
import tensorflow as tf

hidden_dim = 2
num_classes = 10
batch_size = 100  # должно быть кратно 60 000 и 10 0000

(x_train, y_train), (x_test, y_test) = mnist.load_data()

# стандартизация входных данных
x_train = x_train / 255
x_test = x_test / 255

x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

y_train_cat = keras.utils.to_categorical(y_train, num_classes)
y_test_cat = keras.utils.to_categorical(y_test, num_classes)

def dropout_and_batch(x):
  return Dropout(0.3)(BatchNormalization()(x))

input_img = Input(shape=(28, 28, 1))
fl = Flatten()(input_img)
lb = Input(shape=(num_classes,))
x = concatenate([fl, lb])
x = Dense(256, activation='relu')(x)
x = dropout_and_batch(x)
x = Dense(128, activation='relu')(x)
x = dropout_and_batch(x)

z_mean2 = Dense(hidden_dim, name='z_mean')(x)  # Даем слою имя
z_log_var = Dense(hidden_dim, name='z_log_var')(x) # Даем слою имя

def noiser(args):
  z_mean, z_log_var = args
  N = K.random_normal(shape=(K.shape(z_mean)[0], hidden_dim), mean=0., stddev=1.0)  # K.shape для batch_size
  return K.exp(z_log_var / 2) * N + z_mean

h = Lambda(noiser, output_shape=(hidden_dim,))([z_mean2, z_log_var])

input_dec = Input(shape=(hidden_dim,))
lb_dec = Input(shape=(num_classes,))
d = concatenate([input_dec, lb_dec])
d = Dense(128, activation='elu')(d)
d = dropout_and_batch(d)
d = Dense(256, activation='elu')(d)
Урок16.........................................................................................
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import matplotlib.pyplot as plt
import time

from tensorflow.keras.datasets import mnist
import tensorflow.keras.backend as K
import tensorflow as tf
from tensorflow.keras.layers import Dense, Flatten, Reshape, Input, BatchNormalization, Dropout
from tensorflow.keras.layers import Conv2D, Conv2DTranspose, LeakyReLU

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train[y_train == 7]
y_train = y_train[y_train == 7]

BUFFER_SIZE = x_train.shape[0]
BATCH_SIZE = 100

BUFFER_SIZE = BUFFER_SIZE // BATCH_SIZE * BATCH_SIZE
x_train = x_train[:BUFFER_SIZE]
y_train = y_train[:BUFFER_SIZE]
print(x_train.shape, y_train.shape)

# стандартизация входных данных
x_train = x_train / 255
x_test = x_test / 255

x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))
x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))

train_dataset = tf.data.Dataset.from_tensor_slices(x_train).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)

# формирование сетей
hidden_dim = 2

def dropout_and_batch():
  return Dropout(0.3)(BatchNormalization())

# генератор
generator = tf.keras.Sequential([
  Dense(7 * 7 * 256, activation='relu', input_shape=(hidden_dim,)),
  BatchNormalization(),
  Reshape((7, 7, 256)),
  Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', activation='relu'),
  BatchNormalization(),
  Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', activation='relu'),
  BatchNormalization(),
  Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', activation='sigmoid'),
])

# дискриминатор
discriminator = tf.keras.Sequential()
discriminator.add(Conv2D(64, (5, 5), strides=(2, 2), padding='same', input_shape=[28, 28, 1]))
discriminator.add(LeakyReLU())
discriminator.add(Dropout(0.3))

discriminator.add(Conv2D(128, (5, 5), strides=(2, 2), padding='same'))
discriminator.add(LeakyReLU())
discriminator.add(Dropout(0.3))

discriminator.add(Flatten())
discriminator.add(Dense(1))

# потери
cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)

def generator_loss(fake_output):
  loss = cross_entropy(tf.ones_like(fake_output), fake_output)
  return loss

def discriminator_loss(real_output, fake_output):
  real_loss = cross_entropy(tf.ones_like(real_output), real_output)
  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)
  total_loss = real_loss + fake_loss
  return total_loss

generator_optimizer = tf.keras.optimizers.Adam(1e-4)
discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)

# обучение
@tf.function
def train_step(images):
  noise = tf.random.normal([BATCH_SIZE, hidden_dim])

  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:
    generated_images = generator(noise, training=True)

    real_output = discriminator(images, training=True)
    fake_output = discriminator(generated_images, training=True)

    gen_loss = generator_loss(fake_output)
    disc_loss = discriminator_loss(real_output, fake_output)

  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)
  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)

  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))
  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))

  return gen_loss, disc_loss

def train(dataset, epochs):
  history = []
  MAX_PRINT_LABEL = 10
  th = BUFFER_SIZE // (BATCH_SIZE * MAX_PRINT_LABEL)

  for epoch in range(1, epochs + 1):
    print(f'{epoch}/{EPOCHS}: ', end='')

    start = time.time()
    n = 0

    gen_loss_epoch = 0
    for image_batch in dataset:
      gen_loss, disc_loss = train_step(image_batch)
      gen_loss_epoch += K.mean(gen_loss)
      if (n % th == 0): print('=', end='')
      n += 1

    history += [gen_loss_epoch / n]
    print(': ' + str(history[-1]))
    print('Время эпохи {} составляет {} секунд'.format(epoch, time.time() - start))

  return history

# запуск процесса обучения
EPOCHS = 20
history = train(train_dataset, EPOCHS)

plt.plot(history)
plt.grid(True)
plt.show()

# отображение результатов генерации
n = 2
total = 2 * n + 1
plt.figure(figsize=(total, total))
num = 1
for i in range(-n, n + 1):
  for j in range(-n, n + 1):
    ax = plt.subplot(total, total, num)
    num += 1
    img = generator.predict(np.expand_dims([0.5 * i / n, 0.5 * j / n], axis=0))
    plt.imshow(img[0, :, :, 0], cmap='gray')
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()
